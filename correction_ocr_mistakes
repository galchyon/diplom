!pip install deeppavlov
pip install kenlm
pip install sacremoses

import os
from google.colab import drive
import zipfile
import pandas as pd
import numpy as np
import xml.etree.ElementTree as ET
import glob
from deeppavlov import build_model, configs
  
drive.mount('/content/gdrive/')
!unzip -q /content/gdrive/My\ Drive/2014.zip -d train

#сначала загрузим документы из одного года, архив лежит на гугл-диске

year = '2014' # valid years 1991-
datasetdir = 'train/' #your path here

#different types of work for different types of data
fieldsid = ['authorByOP',  'docDateByOP',  'docNumberByOP',   'pravogovruNd',  'IssuedByIPS',  'DocDateIPS' ,  'DocNumberIPS']
fieldsop = ['opdate',  'opnumber', 'opweekcode']
fieldshd = ['headingIPS', 'docTitleByOP', 'docTypeByOP']
fieldstxt = ['textOCR',  'textIPS']
fieldsclass = ['classifierByIPS',  'keywordByIPS']
alldata = [] #we will collect dictionaries to this in order to make a Data Frame of it

#iterate through XML files, parse a file into a dictionary and add to alldata
for inputfilename in glob.glob(datasetdir + str(year) + '*.xml'):
    tree = ET.parse(inputfilename)
    root = tree.getroot()
    bufferdict = {}
    bufferdict['fileid'] = inputfilename[-15:]
    for field in fieldsid:
        try:
            bufferdict[field] = root.find("./meta/identification/" + field).get('val')
        except:
            bufferdict[field] = None

    for field in fieldsop:
        try:
            bufferdict[field] = root.find("./meta/publication/").get(field)
        except:
            bufferdict[field] = None

    for field in fieldshd:
        try:
            bufferdict[field] = root.find("./meta/identification/" + field).text
        except:
            bufferdict[field] = None

    for field in fieldstxt:
        try:
            bufferdict[field] = root.find("./body/" + field).text
        except:
            bufferdict[field] = None
    #then since we can have multiple values from classifier and keywords field we have to represent lists of them via semicolon
    kwall = ''
    try:
        kw = root.findall("./meta/keywords/keywordByIPS")
        for i in kw:
            kwall = kwall + ';' + i.get('val')
        if kwall[0] == ';':
            bufferdict['keywordByIPS'] = kwall[1:]
    except:
        bufferdict['keywordByIPS'] = None
    clasall = ''
    try:
        cla = root.findall("./meta/references/classifierByIPS")
        for i in cla:
            clasall = clasall + ';' + i.get('val')
        if clasall[0] == ';':
            bufferdict['classifierByIPS'] = clasall[1:]
    except:
        bufferdict['classifierByIPS'] = None

    alldata.append(bufferdict)

# finally make a dataframe
df = pd.DataFrame(alldata,  columns = ['fileid']+ fieldsid + fieldsop + fieldshd + fieldstxt + fieldsclass)
# print some results:
print(df.info())

#чистка текстов от нанов, символов табуляции
df_to_work = df.copy()
df_to_work = df_to_work[['docTypeByOP', 'textOCR']]
df_to_work['textOCR'] = df_to_work['textOCR'].str.replace('\n', ' ')
df_to_work['textOCR'] = df_to_work['textOCR'].str.replace('-', ' ')
df_to_work=df_to_work.dropna()
texts = df_to_work['textOCR'].tolist()
df_to_work


#в тексте много опечаток и странных символов, идея - найти все эти символы во всех текстах документов и сохранить их в отдельный сет, вывести его и положить символы в функцию, которая почистит текст от них
#функция для поиска всех плохих символов - то есть тех, которые не являются русскими буквами, пробелом или точкой
def find_bad_symb(l1, l2):
  bad_symb = set()
  for i in range(len(l1)):
    bad_symb.update(set(l1[i])-set(l2))
  return bad_symb


russian_letters = [chr(i) for i in range(0x0410, 0x044F+1)]
russian_letters.append(' ')
russian_letters.append('.')
for i in range(0,11):
  russian_letters.append(str(i))

all_bed = set()
for i in texts:
  all_bed.update(find_bad_symb(i, russian_letters))
all_bed


#теперь можно прямо перечислить все эти символы и положить их в функцию, которая очистит от них текст
def remove_bad_symbols(string):
  bad_symb = re.compile("["
                u"\№"  # emoticons
                u"\і"
                u"\ё"
                u"\{"
                u"\›"
                u"\`"
                u"\“"
                u"\№"
                u"\@"
                u"\—"
                u"\/"
                u"\°"
                u"\«"
                u"\,"
                u"\)"
                u"\…"
                u"\>"
                u"\="
                u"\#"
                u"\:"
                u"\і"
                u"\%"
                u"\]"
                u"\?"
                u"\‘"
                u"\_"
                u"\;"
                u"\["
                u"\*"
                u"\'"
                u"\""
                u"\$"
                u"\<"
                u"\^"
                u"\‚"
                u"\‹"
                u"\’"
                u"\”"
                u"\»"
                u"\&"
                u"\Ё"
                u"\("
                u"\„"
                u"\!"
                u"\|"
                u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                u"\U0001F680-\U0001F6FF"  # transport & map symbols
                u"\U0001F1E0-\U0001F1FF"
                u"\U00002702-\U000027B0"
                u"\U000024C2-\U0001F251"
                u"\U0001f926-\U0001f937"
                u'\U00010000-\U0010ffff'
                u"\u200d"
                u"\u2640-\u2642"
                u"\u2600-\u2B55"
                u"\u23cf"
                u"\u23e9"
                u"\u231a"
                u"\u3030"
                u"\ufe0f"
                 "]+", flags=re.UNICODE)
    return bad_symb.sub(r'', string)


data_test = df_to_work.copy()
data_test['textOCR'] = df_to_work['textOCR'].apply(remove_bad_symbols)
data_test = data_test[['docTypeByOP', 'textOCR']]
data_test

#в текстах все равно сохранились опечатки в некоторых словах. Для того, чтобы исправить хотя бы одну ошибку в слове, прогоним текст через модель от DeepPavlov - spelling corrector на основе метрики Левенштейна

model = build_model('levenshtein_corrector_ru', download=True)

def spelling_correction(col):
  return model([col])

df_corrected = data_test.copy()

df_corrected['textOCR'] = df_corrected['textOCR'].apply(spelling_correction)
df_corrected
